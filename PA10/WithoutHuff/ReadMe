This is my submission for the bonus problems. I have attempted everything, and included two folders worth of  files.

WithoutHuff: this is the folder that contains the working program that searches by prefix, word/words, and will work no matter what the order of reviews.tsv is, as long as the reviews are still grouped by id. If the reviews are completely random and aren't grouped this program will only return the first review for a given business ID/location. To run, compile, and main will explain its own use in stdout (just run it).
Includes:
answer10bonus.h
answer10bonus.c
main.c

WithHuff: the program in this folder in theory can function the same as that of WithoutHuff. For elaboration see "Huffman Theory" below. This program may work with reviews.short.tsv, depending one the amount of memory in the computer, but will almost certianly not work with with reviews.tsv unless you run it on a supercomputer... For elaboration see "Huffman Theory" below. To run, compile, and main will explain its own use in stdout (just run it). By default this does not print the reviews after a search becase it made my computer run out of memory... To change this decomment line 249 of mainhuff.c... I don't recommend this.
Includes:
dostuffhuff.h
dostuffhuff.c
huffer.c
mainhuff.c

Huffman Theory
I find it curious that we should attempt huffman compression by string. It is curious because huffman compression by string an never be as efficient as normal ascii encoding. Encoding anything using the huffman technique/algorithm by a series of n blocks where n > than the number of ascii values will result in a engorged file, not a compressed file. This is evidenced by the file huffed.txt that is the result of running the files in WithHuff. huffed.txt is the stream of 1's and 0's that represents the encoded file. It will be in the WithHuff folder, along with a dehuffed text file to prove that I implemented a dehuffer, and that my huffman encoding by string is correct. I suggest running this with reviews.short.tsv to see huffed.txt, and dehuffed.txt. Anything larger will inflate by a lot (reviews.short.tsv went from 1.1Mb to 79.2Mb!!). While I know the compression ratio is actually a magnitude of order less than that mentioned above becase that is encoded to ascii 1's and 0's, not bits. Still, huffman encoding by strings will result in a much larger file. This is inescapable due to how huffman encoding works. I knew this while I was trying to make this program work, and I eventually reached the limits of the computing memory at my disposal. I tried many avenues to make my program more memory efficient, sinking will over 20 hours total into the this last part of the bonus. Huffman encoding by string is simply too inefficient.

Huffman encoding is more efficient than ascii because the average number of bits used to encode a given character will be less than that of ascii. This works by using fewer bits to store more frequently used characters. Each character is given a binary code such that no character's code is a prefix of another's. This binary code is found by traversing a huffman tree using the code :left = 0, right = 1 to a given character, and saving the path. The height of the huffman tree represents the max number of bits used to encode a block. When huffman encoding is done by character, this height is on the magnitude of order as the number of bits used for ascii encoding. In other words, when encoding by character, the maximum number of bits used to encode a character rarely exceeds that of ascii, and the minimum number of bits is 1, and that is what is used for the most frequently occuring character. This is why the average number of bits for a character in huffman encoding is less than that of ascii. All of this is torn apart when encoding by string. The number of unique strings is... magnitudes of order larger than the number of characters. This automatically makes huffman encoding inefficient becaue as the number of blocs in a huffman tree increases, the tree's height, and therefore max bits for a block increases in size. With the amount of unique words in reviews.tsv, the max bits for a word is massive. So much so that it exceeds the possible length of any word in the file. This increases the average number of bits for a bloc to far far far more than that of ascii. For this reason, huffman encoding by string is completely inefficient. In the case of reviews.tsv, and even some times reviews.short.tsv. the encoding is so inefficient that my computers run out of memory holding the tree and using it to parse the huffed reviews.tsv at the same time. I have worked so hard to try to find a way around this, sunk hours upon hours into it, and I could not achieve success.


